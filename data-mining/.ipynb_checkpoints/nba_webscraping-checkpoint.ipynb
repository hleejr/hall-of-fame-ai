{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036b50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used for webscraping\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocess import Pool\n",
    "import sys, requests, pandas, numpy\n",
    "\n",
    "# any config/setup for imported libraries\n",
    "pandas.options.display.max_columns = None\n",
    "pandas.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1d9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to simplify pulling all player data based on Lebron test\n",
    "def scrape_profile(url):\n",
    "    '''\n",
    "    This function starts by using the request library which creates the request to a html page based on the url parameter. Next it uses beautiful soup to transform the response data into a format that can be accessed and modified. After retrieving the players name from the html, the next two lines filter the response for tables holding that player's career totals in the regular season and the playoffs. The next compound statement focuses on retrieving information about any awards they've has received throughout their career. After locating the html by id, the information is transformed and stored. Moving to the next block of code, the parsed html data is manipulated to retrieve the pieces of text that are important for collection purposes. First, the names of each stat is retrieved and stored. Then, the unneeded columns are sliced from the new list of stat names.\n",
    "\n",
    "    The third compound statement actually starts the data mining. The table containing regular season totals that was found is filtered and transformed by creating a new list from every row that is present in the table. Each row represents another season the player was active in their league. Each of the three pieces of extracted data (regular seasons stats, playoff stats, and awards) are stored in their own variable and returned.\n",
    "    '''\n",
    "    \n",
    "    player_page = requests.get(url)\n",
    "    player_soup = BeautifulSoup(player_page.content, 'html.parser')\n",
    "    player_name = player_soup.find(id='meta').find(\"h1\").find(\"span\").text\n",
    "    player_totals = player_soup.find(id=\"totals\")\n",
    "    player_playoffs_totals = player_soup.find(id=\"playoffs_totals\")\n",
    "\n",
    "    html_awards = player_soup.body.find_all(id='bling')\n",
    "    player_awards = []\n",
    "    player_awards.append(\"awards\")\n",
    "    player_awards.append(player_name)\n",
    "    if html_awards:\n",
    "        award = html_awards[0].find_all('a')\n",
    "        for i in range(len(award)):\n",
    "            player_awards.append(award[i].text)\n",
    "\n",
    "    table_columns = player_totals.find_all(\"th\")\n",
    "    stat_columns = []\n",
    "    for stat in table_columns:\n",
    "        if stat.text == \"\\xa0\":\n",
    "            continue\n",
    "        stat_columns.append(stat.text)\n",
    "    stat_names = stat_columns[0:31]\n",
    "    \n",
    "    stat_names.insert(1, \"Name\")\n",
    "\n",
    "    player_reg = []\n",
    "    if player_totals is not None:\n",
    "        reg_table = player_totals.find(\"tbody\").find_all(\"tr\")\n",
    "        for row in reg_table:\n",
    "            season=[]\n",
    "            season.append(\"reg\")\n",
    "            season.append(player_name)\n",
    "            season.append(row.find(\"th\").text)\n",
    "            for stat in row.find_all(\"td\"):\n",
    "                if stat.text == \"\":\n",
    "                    continue\n",
    "                season.append(stat.text)\n",
    "            player_reg.append(season)\n",
    "\n",
    "    player_po = []\n",
    "    if player_playoffs_totals is not None:\n",
    "        po_table = player_playoffs_totals.find(\"tbody\").find_all(\"tr\")\n",
    "        for row in po_table:\n",
    "            season=[]\n",
    "            season.append(\"post\")\n",
    "            season.append(player_name)\n",
    "            season.append(row.find(\"th\").text)\n",
    "            for stat in row.find_all(\"td\"):\n",
    "                if stat.text == \"\":\n",
    "                    continue\n",
    "                season.append(stat.text)\n",
    "            player_po.append(season)\n",
    "        \n",
    "    return player_reg, player_po, player_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links = []\n",
    "\n",
    "for x in range(97,123): # loop over every letter of the alphabet\n",
    "    char = chr(x)\n",
    "    page = requests.get(f'https://www.basketball-reference.com/players/{char}')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    player_links = soup.find_all('tr')\n",
    "    \n",
    "    for i in range(len(player_links)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else: \n",
    "            player_link = player_links[i].find('a', href=True)['href']\n",
    "            player_url = f'https://www.basketball-reference.com/{player_link}'\n",
    "            links.append(player_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093beed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# using multiprocess to speed up scraping\n",
    "\n",
    "with Pool(10) as p:\n",
    "    records = p.map(scrape_profile, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e391b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save records into three seperate pandas dataframes: one for the regular season, one for the post season and one for awards\n",
    "\n",
    "col_names = stat_columns[0:31]\n",
    "col_names.insert(0, \"type\")\n",
    "col_names.insert(1, \"Name\")\n",
    "\n",
    "reg_season = pandas.DataFrame(columns = col_names)\n",
    "po_season = pandas.DataFrame(columns = col_names)\n",
    "awards = pandas.DataFrame()\n",
    "\n",
    "for record in records:\n",
    "    reg, po, aw = record\n",
    "    add_rg = pandas.DataFrame(reg)\n",
    "    add_po = pandas.DataFrame(po)\n",
    "    add_aw = pandas.DataFrame(aw)\n",
    "    reg_season = pandas.concat([reg_season, add_rg], sort=False)\n",
    "    po_season = pandas.concat([po_season, add_po], sort=False)\n",
    "    awards = pandas.concat([awards, add_aw], sort=False)\n",
    "    \n",
    "reg_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eea72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
